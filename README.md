# Scraper de la Gaceta Oficial de Bolivia

Scraper completo para la Gaceta Oficial de Bolivia que extrae documentos legales, los parsea y estructura con metadatos completos.

**IMPORTANTE:** El sitio oficial bloquea requests HTTP simples. Este scraper usa **Selenium** para simular un navegador real.

## Caracter√≠sticas

- **Dos modos de scraping:**
  - **Selenium (recomendado):** Simula navegador real, evita bloqueos
  - **Requests:** HTTP directo, r√°pido pero puede fallar

- **Scraping robusto:**
  - Navegaci√≥n autom√°tica de p√°ginas
  - Delays aleatorios anti-detecci√≥n
  - Manejo de errores y reintentos
  - Descarga masiva de PDFs

- **Extracci√≥n de texto:**
  - PyPDF2 para PDFs digitales
  - OCR (Tesseract) para PDFs escaneados
  - Fallback autom√°tico

- **Parsing de secciones jur√≠dicas:**
  - VISTOS
  - CONSIDERANDO
  - POR TANTO
  - ART√çCULOS (con n√∫meros)
  - DISPOSICIONES (FINALES, TRANSITORIAS, ADICIONALES, ABROGATORIAS)

- **Extracci√≥n de metadatos:**
  - Tipo de norma (LEY, DECRETO SUPREMO, etc.)
  - N√∫mero de norma
  - Fecha (normalizada a ISO)
  - Entidad emisora
  - Temas principales

- **Exportaci√≥n:**
  - JSON (con texto completo y art√≠culos)
  - CSV (metadatos estructurados)

## Estructura del Proyecto

```
bo-scraper-gaceta/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ QUICKSTART.md        # Gu√≠a r√°pida
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ main.py              # üéØ PUNTO DE ENTRADA PRINCIPAL
‚îú‚îÄ‚îÄ selenium_scraper.py  # Scraper con Selenium (navegador real)
‚îú‚îÄ‚îÄ scraper.py           # Scraper con requests (HTTP directo)
‚îú‚îÄ‚îÄ parser.py            # Parsing de documentos legales + OCR
‚îú‚îÄ‚îÄ metadata.py          # Extracci√≥n de metadatos
‚îú‚îÄ‚îÄ config.py            # Configuraci√≥n
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/            # HTML guardado para debug
‚îÇ   ‚îú‚îÄ‚îÄ pdfs/           # PDFs descargados
‚îÇ   ‚îî‚îÄ‚îÄ text/           # Textos extra√≠dos
‚îÇ
‚îú‚îÄ‚îÄ exports/
‚îÇ   ‚îú‚îÄ‚îÄ json/           # Exportaciones JSON
‚îÇ   ‚îî‚îÄ‚îÄ csv/            # Exportaciones CSV
‚îÇ
‚îî‚îÄ‚îÄ scripts/
    ‚îî‚îÄ‚îÄ run_full.py     # Script legacy (modo requests)
```

## Instalaci√≥n

### 1. Clonar el repositorio

```bash
git clone https://github.com/zambogram/scraper.py.git
cd scraper.py
```

### 2. Instalar dependencias Python

```bash
pip install -r requirements.txt
```

### 3. Instalar ChromeDriver (OBLIGATORIO para modo Selenium)

#### Ubuntu/Debian:
```bash
sudo apt-get update
sudo apt-get install chromium-chromedriver

# Verificar instalaci√≥n
chromedriver --version
```

#### MacOS:
```bash
brew install --cask chromedriver

# O con Homebrew:
brew install chromedriver

# Verificar instalaci√≥n
chromedriver --version
```

#### Windows:
1. Descarga ChromeDriver desde: https://chromedriver.chromium.org/downloads
2. **IMPORTANTE:** Descarga la versi√≥n que coincida con tu Chrome instalado
3. Extrae el archivo `chromedriver.exe`
4. A√±√°delo al PATH o especifica la ruta con `--chromedriver-path`

Para verificar tu versi√≥n de Chrome:
```bash
# En Chrome, ve a: chrome://version/
# O desde terminal:
google-chrome --version  # Linux
"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome" --version  # MacOS
```

### 4. (Opcional) Instalar Tesseract para OCR

Si los PDFs est√°n escaneados (im√°genes), necesitar√°s OCR:

#### Ubuntu/Debian:
```bash
sudo apt-get install tesseract-ocr tesseract-ocr-spa
```

#### MacOS:
```bash
brew install tesseract tesseract-lang
```

#### Windows:
1. Descarga desde: https://github.com/UB-Mannheim/tesseract/wiki
2. Instala e incluye el paquete de idioma espa√±ol

## Uso

### üöÄ Comando m√°s simple (modo prueba)

```bash
python main.py --test
```

Esto scrapear√° **10 documentos** para verificar que todo funciona.

### ‚ö° Comandos Principales

```bash
# Modo Selenium (RECOMENDADO) - Scrapear 5 p√°ginas
python main.py --modo selenium --paginas 5

# Scrapear TODO (cuidado: puede tardar HORAS)
python main.py --modo selenium --paginas 9999

# Modo prueba (10 documentos)
python main.py --test

# Scrapear SIN descargar PDFs (solo metadatos)
python main.py --modo selenium --paginas 5 --no-download-pdfs

# Ver el navegador Chrome (√∫til para debugging)
python main.py --modo selenium --no-headless --limite 5

# URL personalizada
python main.py --modo selenium --url "http://www.gacetaoficialdebolivia.gob.bo/normas/listadonor/10"

# ChromeDriver personalizado
python main.py --modo selenium --chromedriver-path /usr/local/bin/chromedriver
```

### üìã Opciones Completas

```
main.py [-h] [--modo {selenium,requests}] [--paginas PAGINAS]
        [--limite LIMITE] [--url URL] [--headless] [--no-headless]
        [--chromedriver-path PATH] [--download-pdfs] [--no-download-pdfs]
        [--test] [--full]

Opciones:
  --modo {selenium,requests}  Modo de scraping (default: selenium)
  --paginas PAGINAS          P√°ginas a scrapear (9999=todas, default: 1)
  --limite LIMITE            L√≠mite de documentos (0=todos, default: 0)
  --url URL                  URL base personalizada
  --headless                 Chrome sin interfaz (default: True)
  --no-headless              Mostrar Chrome (√∫til para debug)
  --chromedriver-path PATH   Ruta a chromedriver
  --download-pdfs            Descargar PDFs (default: True)
  --no-download-pdfs         Solo metadatos, no descargar PDFs
  --test                     Modo prueba: 10 docs
  --full                     Scrapear TODO
```

## Formato de Salida

### JSON
```json
{
  "id": "ley_1234_20240115",
  "titulo": "Ley de...",
  "tipo_norma": "LEY",
  "numero_norma": "1234",
  "fecha": "2024-01-15",
  "seccion": "LEY",
  "entidad_emisora": "ASAMBLEA LEGISLATIVA",
  "url_pdf": "http://...",
  "resumen": "...",
  "temas": "EDUCACI√ìN,SALUD",
  "num_articulos": 25,
  "num_considerandos": 5,
  "texto_completo": "...",
  "articulos_json": "[{\"numero\":\"1\",\"contenido\":\"...\"}]"
}
```

### CSV
Columnas: id, titulo, tipo_norma, numero_norma, fecha, seccion, entidad_emisora, url_pdf, resumen, temas, num_articulos, num_considerandos, tiene_vistos, tiene_disposiciones_finales

## Soluci√≥n de Problemas

### Error: "chromedriver executable needs to be in PATH"

**Soluci√≥n:**
```bash
# Opci√≥n 1: Especificar ruta manualmente
python main.py --chromedriver-path /ruta/a/chromedriver

# Opci√≥n 2: Instalar con gestores de paquetes (ver secci√≥n Instalaci√≥n)

# Opci√≥n 3: Agregar al PATH
export PATH=$PATH:/ruta/donde/esta/chromedriver  # Linux/Mac
```

### Error: "This version of ChromeDriver only supports Chrome version XX"

**Soluci√≥n:**
1. Verifica tu versi√≥n de Chrome: `google-chrome --version`
2. Descarga ChromeDriver compatible desde: https://chromedriver.chromium.org/downloads
3. Reemplaza el chromedriver antiguo

### No se encuentran documentos

**Soluci√≥n:**
1. Verifica que el sitio est√© funcionando: http://www.gacetaoficialdebolivia.gob.bo/
2. El scraper guardar√° un HTML en `data/raw/debug_html_*.html` para que inspecciones la estructura
3. Ajusta los selectores en `selenium_scraper.py` funci√≥n `_extraer_documentos_de_pagina_actual()`

### PDFs descargados pero sin texto

**Posibles causas:**
- PDF es escaneado (imagen), no digital

**Soluci√≥n:**
1. Instala Tesseract OCR (ver secci√≥n Instalaci√≥n)
2. El scraper usar√° autom√°ticamente OCR como fallback

### El sitio me bloquea

**Soluci√≥n:**
- Ya est√°s usando Selenium que simula un navegador real
- Aumenta los delays en `selenium_scraper.py`:
  ```python
  self._random_delay(min_sec=3, max_sec=7)  # M√°s delay
  ```

### Error 503 Service Unavailable

**Causas posibles:**
- El sitio est√° ca√≠do temporalmente
- Demasiadas requests simult√°neas

**Soluci√≥n:**
- Espera unos minutos
- Usa `--limite` bajo para probar: `--limite 5`
- Aumenta delays entre requests

## Configuraci√≥n Avanzada

Edita `config.py` para ajustar:
- URLs del sitio
- Timeouts y reintentos
- Tipos de normas reconocidas
- Secciones jur√≠dicas esperadas

## Ajuste de Selectores HTML

Si el sitio cambia su estructura o el scraper no encuentra documentos:

1. El scraper guardar√° HTML en `data/raw/debug_html_*.html`
2. Abre ese archivo e inspecciona la estructura
3. Edita `selenium_scraper.py`, funci√≥n `_extraer_documentos_de_pagina_actual()` (l√≠nea ~170)
4. Ajusta los selectores CSS seg√∫n la estructura real

El scraper intenta **4 estrategias** autom√°ticamente:
- Estrategia 1: Tablas (`<table>`)
- Estrategia 2: Divs con clases espec√≠ficas
- Estrategia 3: Enlaces directos a PDF
- Estrategia 4: Inspecci√≥n gen√©rica

## Ejemplos de Uso Completo

### Caso 1: Prueba R√°pida (10 documentos)
```bash
python main.py --test
```

### Caso 2: Scrapear un mes completo (estimado: 50-100 docs)
```bash
python main.py --modo selenium --paginas 10
```

### Caso 3: Scrapear TODO sin descargar PDFs (solo metadatos)
```bash
python main.py --full --no-download-pdfs
```

### Caso 4: Debugging (ver Chrome en acci√≥n)
```bash
python main.py --no-headless --limite 5
```

### Caso 5: Servidor sin interfaz gr√°fica
```bash
python main.py --headless --paginas 20
```

## Dependencias

**Core:**
- requests: HTTP requests (modo requests)
- beautifulsoup4: HTML parsing (modo requests)
- lxml: Parser r√°pido
- PyPDF2: Extracci√≥n de texto de PDF
- python-dateutil: Parsing de fechas

**Selenium (navegador real):**
- selenium: Automatizaci√≥n de navegador
- webdriver-manager: Gesti√≥n autom√°tica de ChromeDriver (opcional)

**OCR (opcional, para PDFs escaneados):**
- pytesseract: Wrapper de Tesseract
- Pillow: Procesamiento de im√°genes
- pdf2image: Conversi√≥n PDF a imagen

## Logs

Los logs se guardan autom√°ticamente:
```
gaceta_scraper_20240115_143022.log
```

Nivel de logging: INFO (configurable en `config.py`)

## Limitaciones Conocidas

1. **Velocidad:** Selenium es m√°s lento que requests (pero m√°s robusto)
2. **Dependencia de Chrome:** Requiere Chrome y ChromeDriver instalados
3. **Selectores HTML:** Pueden cambiar si el sitio se actualiza
4. **PDFs escaneados:** OCR es lento y puede tener errores
5. **Memoria:** Procesar miles de PDFs puede consumir mucha RAM

## Rendimiento Estimado

- **Modo Selenium + PyPDF2:** ~10-20 documentos/minuto
- **Modo Selenium + OCR:** ~2-5 documentos/minuto (mucho m√°s lento)
- **Memoria:** ~500MB-2GB dependiendo de PDFs

## Recomendaciones

‚úÖ Usa `--test` primero para verificar funcionamiento
‚úÖ Empieza con `--limite` bajo antes de scrapear todo
‚úÖ Usa `--headless` en servidores
‚úÖ Revisa los logs para detectar problemas
‚úÖ Haz backups de `exports/` regularmente

‚ùå No uses `--paginas 9999` sin antes probar con l√≠mites bajos
‚ùå No ejecutes m√∫ltiples instancias simult√°neas (puede causar bloqueos)
‚ùå No ignores los mensajes de advertencia

## Soporte

- Reporta issues en: [GitHub Issues](https://github.com/zambogram/scraper.py/issues)
- Consulta `QUICKSTART.md` para comandos r√°pidos

## Licencia

[Especificar licencia]

## Contribuciones

[Instrucciones para contribuir]

---

**Desarrollado para la Gaceta Oficial del Estado Plurinacional de Bolivia**
Sitio oficial: http://www.gacetaoficialdebolivia.gob.bo/
